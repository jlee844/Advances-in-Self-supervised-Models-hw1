{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bf44897",
   "metadata": {},
   "source": [
    "\n",
    "# Section 2.3 Perplexity & Sampling Analysis (Fixed Functions & Prints)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "515010d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import warnings\n",
    "import re\n",
    "from typing import Dict\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd83dc11",
   "metadata": {},
   "source": [
    "# Import Distilgpt2 Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d30b07b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on: cpu\n",
      "Model parameters: 81,912,576\n",
      "Vocabulary size: 50,257\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on: {device}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912da2c4",
   "metadata": {},
   "source": [
    "# Perplexity and paragraph shuffle function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3f8ab81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_perplexity(text: str, model: GPT2LMHeadModel, tokenizer: GPT2Tokenizer, device: torch.device):\n",
    "    enc = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = enc[\"input_ids\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=input_ids, labels=input_ids)\n",
    "        loss = out.loss\n",
    "    ppl = torch.exp(loss).item()\n",
    "    return ppl, float(loss)\n",
    "\n",
    "def shuffle_text(text: str):\n",
    "    sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "    shuffled = sentences.copy()\n",
    "    random.shuffle(shuffled)\n",
    "    joined = '. '.join(shuffled)\n",
    "    if joined and not joined.endswith('.'):\n",
    "        joined += '.'\n",
    "    return joined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ae6741",
   "metadata": {},
   "source": [
    "# Part a) perplexity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d2b05c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unanimous Declaration of the thirteen united States of America, When in the Course of human events, it becomes necessary for one people to dissolve the political bands which have connected them with another, and to assume among the powers of the earth, the separate and equal station to which the Laws of Nature and of Nature's God entitle them, a decent respect to the opinions of mankind requires that they should declare the causes which impel them to the separation. We hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness.--That to secure these rights, Governments are instituted among Men, deriving their just powers from the consent of the governed, --That whenever any Form of Government becomes destructive of these ends, it is the Right of the People to alter or to abolish it, and to institute new Government, laying its foundation on such principles and organizing its powers in such form, as to them shall seem most likely to effect their Safety and Happiness.\n",
      "\n",
      "\n",
      "Original perplexity: 24.71 | loss: 3.2073\n",
      "Shuffled  perplexity: 26.89 | loss: 3.2918\n",
      "Ratio (shuffled / original): 1.09\n",
      "Loss difference (shuffled - original): 0.0845\n",
      "Shuffled preview: The unanimous Declaration of the thirteen united States of America, When in the Course of human events, it becomes necessary for one people to dissolve the political bands which have connected them wi...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_paragraph = (\n",
    "    \"The unanimous Declaration of the thirteen united States of America, \"\n",
    "    \"When in the Course of human events, it becomes necessary for one people to \"\n",
    "    \"dissolve the political bands which have connected them with another, and to assume among \"\n",
    "    \"the powers of the earth, the separate and equal station to which the Laws of Nature and of Nature's God entitle them, \"\n",
    "    \"a decent respect to the opinions of mankind requires that they should declare the causes which impel them to the separation. We hold these truths \"\n",
    "    \"to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, \"\n",
    "    \"Liberty and the pursuit of Happiness.--That to secure these rights, Governments are instituted among Men, deriving their just powers from the consent of the \"\n",
    "    \"governed, --That whenever any Form of Government becomes destructive of these ends, it is the Right of the People to alter or to abolish it, and to institute new Government, \"\n",
    "    \"laying its foundation on such principles and organizing its powers in such form, as to them shall seem most likely to effect their Safety and Happiness.\"\n",
    ")\n",
    "\n",
    "print(test_paragraph)\n",
    "\n",
    "results = []\n",
    "\n",
    "original_perplexity, original_loss = compute_perplexity(test_paragraph, model, tokenizer, device)\n",
    "\n",
    "shuffled_paragraph = shuffle_text(test_paragraph)\n",
    "shuffled_perplexity, shuffled_loss = compute_perplexity(shuffled_paragraph, model, tokenizer, device)\n",
    "\n",
    "result = {\n",
    "    'paragraph_num': i,\n",
    "    'original_text': paragraph,\n",
    "    'shuffled_text': shuffled_paragraph,\n",
    "    'original_perplexity': original_perplexity,\n",
    "    'shuffled_perplexity': shuffled_perplexity,\n",
    "    'original_loss': original_loss,\n",
    "    'shuffled_loss': shuffled_loss,\n",
    "    'perplexity_ratio': (shuffled_perplexity / original_perplexity) if original_perplexity else float('inf'),\n",
    "    'loss_difference': shuffled_loss - original_loss\n",
    "}\n",
    "results.append(result)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"Original perplexity: {original_perplexity:.2f} | loss: {original_loss:.4f}\")\n",
    "print(f\"Shuffled  perplexity: {shuffled_perplexity:.2f} | loss: {shuffled_loss:.4f}\")\n",
    "print(f\"Ratio (shuffled / original): {result['perplexity_ratio']:.2f}\")\n",
    "print(f\"Loss difference (shuffled - original): {result['loss_difference']:.4f}\")\n",
    "print(f\"Shuffled preview: {shuffled_paragraph[:200]}{'...' if len(shuffled_paragraph) > 200 else ''}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e22d74",
   "metadata": {},
   "source": [
    "# Comment on difference part a):\n",
    "\n",
    "The shuffled paragraph’s perplexity is higher than the oriparagraph. This is expected because shuffling disrupts discourse-level structure and some cross-sentence context that the LM relies on for next‑token prediction.  Since we shuffled by sentence, most within‑sentence token dependencies remain intact, so the increase is modest compared to word‑level shuffling. If we instead shuffle words, the increase in perplexity would be a lot higher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4d185b",
   "metadata": {},
   "source": [
    "# Part b) Sampling Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe13401c",
   "metadata": {},
   "source": [
    "## Greedy text generation and with temperature tuning text generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85f6a5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_text_greedy(prompt: str, max_length: int = 500, model: GPT2LMHeadModel = None,\n",
    "                         tokenizer: GPT2Tokenizer = None, device: torch.device = None) -> str:\n",
    "    if model is None:\n",
    "        model = globals()['model']\n",
    "    if tokenizer is None:\n",
    "        tokenizer = globals()['tokenizer']\n",
    "    if device is None:\n",
    "        device = globals()['device']\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out_ids = model.generate(\n",
    "            **enc,\n",
    "            max_length=max_length,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    return tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "\n",
    "def generate_text_temperature(prompt: str, temperature: float = 1.0, max_length: int = 500,\n",
    "                              model: GPT2LMHeadModel = None,\n",
    "                              tokenizer: GPT2Tokenizer = None, device: torch.device = None) -> str:\n",
    "    if model is None:\n",
    "        model = globals()['model']\n",
    "    if tokenizer is None:\n",
    "        tokenizer = globals()['tokenizer']\n",
    "    if device is None:\n",
    "        device = globals()['device']\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out_ids = model.generate(\n",
    "            **enc,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_k=0,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    return tokenizer.decode(out_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b3049f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = \"Once upon a time\"\n",
    "max_length = 500\n",
    "temperatures = [0, 0.3, 0.6, 0.9, 1.2, 1.5]\n",
    "\n",
    "greedy_text = generate_text_greedy(prompt, max_length)\n",
    "generated_texts = {0: greedy_text}\n",
    "\n",
    "for idx, temp in enumerate(temperatures[1:], start=2):\n",
    "    temp_text = generate_text_temperature(prompt, temp, max_length)\n",
    "    generated_texts[temp] = temp_text\n",
    "\n",
    "out_path = Path.cwd() / \"generated_texts_by_temperature.txt\"\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for temp, text in sorted(generated_texts.items(), key=lambda kv: float(kv[0])):\n",
    "        header = \"Greedy (T=0)\" if float(temp) == 0.0 else f\"Temperature {float(temp):.1f}\"\n",
    "        f.write(header + \"\\n\" + \"-\" * len(header) + \"\\n\")\n",
    "        f.write(f\"Prompt: {prompt}\\n\")\n",
    "        f.write(text + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a05f48a",
   "metadata": {},
   "source": [
    "# Compare diversity and quality:\n",
    "\n",
    "\n",
    "Greedy (T=0)\n",
    "Highly repetitive, loops the same sentence (“United States… military presence”). Coherent at the sentence level, but collapses into exact repetition; no progression.\n",
    "\n",
    "T = 0.3\n",
    "Output has slightly more variation than greedy, but still having heavy repetition. Low novelty; minimal narrative development.\n",
    "\n",
    "T = 0.6\n",
    "The output has more lexical variety (“Caspian Empire” for example), but had several repetition in sentence pieces (“was divided into two parts” repeated).\n",
    "\n",
    "T = 0.9\n",
    "Decent diversity and narrative movement (ceasefire, oil fields, characters). Sentence starts to contradicts each other.\n",
    "\n",
    "T = 1.2\n",
    "High creativity in sentences but have severe incoherence and broken logic.\n",
    "\n",
    "T = 1.5\n",
    "Maximum diversity, minimal sense: word salad, entity mashups, formatting glitches, opaque references.\n",
    "\n",
    "In general, diversity increases monotonically with temperature. On the other hand, The quality of the text follows an inverted-U shape. It is  best around 0.6–0.9 for creative text. Low temperature cause a lot of repetition while high temperatures  makes the text incoherent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
